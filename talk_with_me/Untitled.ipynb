{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data4ml import Data4TextGeneration\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time, sleep\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "flags = Namespace(\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['I', 'am'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420/420 [08:37<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "data4gen = Data4TextGeneration(path_to_config='./../data_params.json')\n",
    "data4gen.home_folder = './../../messages'\n",
    "res = data4gen.make_data(limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def make_first_char_upper(sent: list):\n",
    "    sent[0] = sent[0][0].upper() + sent[0][1:]\n",
    "    \n",
    "def add_dot_in_the_end(sent: list):\n",
    "    sent[-1] = sent[-1] + '.' if sent[-1][-1].isalpha() else sent[-1]\n",
    "    \n",
    "def tokenize(corpus: list) -> list:\n",
    "    tokenize_messages = []\n",
    "    for dialog in corpus:\n",
    "        for message in dialog:\n",
    "            sentence = [re.sub(\"[.,:@-]\", \"\", i) for i in message.lower().split() if len(re.sub(\"[.,:@-]\", \"\", i)) > 0]\n",
    "            if sentence:\n",
    "                make_first_char_upper(sentence)\n",
    "                add_dot_in_the_end(sentence)\n",
    "                tokenize_messages.extend(sentence)\n",
    "    return tokenize_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(text, batch_size, seq_size):\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(text: list):\n",
    "    with open('text.txt', 'w') as f:\n",
    "        f.write(' '.join(text))\n",
    "\n",
    "def read_text(file='text.txt') -> list:\n",
    "    with open('text.txt', 'r') as f:\n",
    "        text = f.read().split()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['будешь', 'дома.', 'Чтобы', 'я', 'тебе', 'всё', 'передал.', 'Окей.', 'Ты', 'завтра', 'работать', 'не', 'планируешь', 'что', 'ли?', 'Мы', 'же', 'должны', 'были', 'сегодня']\n"
     ]
    }
   ],
   "source": [
    "text = read_text()\n",
    "print(text[30:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
    "\n",
    "\n",
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))\n",
    "\n",
    "\n",
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer\n",
    "\n",
    "\n",
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "    words = ['Привет', 'как']\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "    \n",
    "    print('сейчас что-то будет...')\n",
    "    print(' '.join(words))\n",
    "    print('было...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 324259\n",
      "iteration = 10\n",
      "Elapsed time: 831.464 sec\n",
      "iteration = 20\n",
      "Elapsed time: 695.964 sec\n",
      "iteration = 30\n",
      "Elapsed time: 738.748 sec\n",
      "iteration = 40\n",
      "Elapsed time: 816.733 sec\n",
      "iteration = 50\n",
      "Elapsed time: 1357.573 sec\n",
      "iteration = 60\n",
      "Elapsed time: 1185.080 sec\n",
      "iteration = 70\n",
      "Elapsed time: 1380.037 sec\n",
      "iteration = 80\n",
      "Elapsed time: 1618.401 sec\n",
      "iteration = 90\n",
      "Elapsed time: 1379.346 sec\n",
      "iteration = 100\n",
      "Elapsed time: 1526.700 sec\n",
      "Epoch: 0/200 Iteration: 100 Loss: 8.92923641204834\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoint_pt/model-100.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bf1af110e735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m                   'Loss: {}'.format(loss_value))\n\u001b[1;32m     52\u001b[0m             torch.save(net.state_dict(),\n\u001b[0;32m---> 53\u001b[0;31m                        'checkpoint_pt/model-{}.pth'.format(iteration))\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint_pt/model-100.pth'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "    text, flags.batch_size, flags.seq_size)\n",
    "\n",
    "net = RNNModule(n_vocab, flags.seq_size,\n",
    "                flags.embedding_size, flags.lstm_size)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "iteration = 0\n",
    "start = time()\n",
    "\n",
    "for e in range(200):\n",
    "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "    state_h, state_c = net.zero_state(flags.batch_size)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "        loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        _ = torch.nn.utils.clip_grad_norm_(\n",
    "            net.parameters(), flags.gradients_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print(f'iteration = {iteration}')\n",
    "            print(\"Elapsed time: {:.3f} sec\".format(time() - start))\n",
    "            start = time()\n",
    "            \n",
    "        if iteration % 100 == 0:\n",
    "            print('Epoch: {}/{}'.format(e, 200),\n",
    "                  'Iteration: {}'.format(iteration),\n",
    "                  'Loss: {}'.format(loss_value))\n",
    "            torch.save(net.state_dict(),\n",
    "                       'checkpoint_pt/model-{}.pth'.format(iteration))\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            predict(device, net, flags.initial_words, n_vocab,\n",
    "                    vocab_to_int, int_to_vocab, top_k=5)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сейчас что-то будет...\n",
      "b'\\xd0\\x9f\\xd1\\x80\\xd0\\xb8\\xd0\\xb2\\xd0\\xb5\\xd1\\x82 \\xd0\\xba\\xd0\\xb0\\xd0\\xba \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xaf \\xd0\\xb8 \\xd0\\xb2 \\xd0\\xaf \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd1\\x8f \\xd0\\xb8 \\xd1\\x8f \\xd0\\xb8 \\xd1\\x8f \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb2 \\xd0\\xaf \\xd0\\xaf \\xd0\\xb8 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb2 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x81 \\xd0\\x9d\\xd1\\x83 \\xd1\\x8f \\xd0\\xb8 \\xd1\\x8f \\xd0\\xb2 \\xd0\\xaf \\xd0\\xb2 \\xd0\\xb2 \\xd0\\xaf \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb2 \\xd0\\xb2 \\xd0\\xaf \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd1\\x8f \\xd1\\x8f \\xd1\\x8f \\xd1\\x8f \\xd0\\xaf \\xd0\\xaf \\xd0\\xb8 \\xd0\\xb8 \\xd0\\xb2 \\xd0\\xb2 \\xd0\\xaf \\xd0\\xb2 \\xd0\\xb8 \\xd0\\xb2 \\xd0\\xb8 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb2 \\xd0\\xb8 \\xd0\\xaf \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\xb8 \\xd0\\xb8 \\xd0\\xb8 \\xd0\\xb8 \\xd0\\xb8 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x81 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x8f \\xd1\\x8f \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb8 \\xd0\\xb8 \\xd0\\xaf \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb2 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xaf \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xaf \\xd1\\x81 \\xd0\\x9d\\xd1\\x83 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb8 \\xd0\\xb2 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xb8 \\xd0\\xb2 \\xd1\\x8f \\xd0\\xb2 \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x81 \\xd0\\x9d\\xd1\\x83 \\xd1\\x87\\xd1\\x82\\xd0\\xbe'\n",
      "было...\n"
     ]
    }
   ],
   "source": [
    "predict(device, net, flags.initial_words, n_vocab,\n",
    "                    vocab_to_int, int_to_vocab, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = []\n",
    "\n",
    "for dialogs in res:\n",
    "    ss = []\n",
    "    for message in dialogs:\n",
    "        ss.extend([x.lower().replace('[,.?]', '') for x in message.split()])\n",
    "    RES.append(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['серега,', 'привету', 'тебя', 'есть', 'телевизор', 'на', 'даче?', 'просто', 'если', 'что,']\n"
     ]
    }
   ],
   "source": [
    "print(RES[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c8a900bad70d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bigram' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(RES, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[RES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321400"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['не', 'Я', 'я', 'и', 'в', 'А', 'Ну', 'И', 'ты', 'на']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.17 mins\n",
      "Time to train the model: 3.78 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_mode = kek(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kek(res):\n",
    "    w2v_model = Word2Vec(min_count=20,\n",
    "                         window=2,\n",
    "                         size=300,\n",
    "                         sample=6e-5, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         negative=20,\n",
    "                         workers=cores-1)\n",
    "\n",
    "    t = time()\n",
    "\n",
    "    w2v_model.build_vocab(res, progress_per=10000)\n",
    "\n",
    "    print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "    w2v_model.corpus_count\n",
    "\n",
    "    t = time()\n",
    "\n",
    "    w2v_model.train(res, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "    print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "    w2v_model.init_sims(replace=True)\n",
    "\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.05 mins\n",
      "Time to train the model: 0.44 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model_1 = kek(RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Написала', 0.8950090408325195),\n",
       " ('Ване', 0.8853252530097961),\n",
       " ('говорит?', 0.8794123530387878),\n",
       " ('ВСЕ', 0.8751375079154968),\n",
       " ('звонила?', 0.8749449253082275),\n",
       " ('Богу', 0.8741962909698486),\n",
       " ('покажи', 0.8736038208007812),\n",
       " ('надоел', 0.8734161853790283),\n",
       " ('Ахахахаах', 0.8711090087890625),\n",
       " ('вк?', 0.8618061542510986)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_mode.wv.most_similar(positive=[\"Юля\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('память', 0.7860834002494812),\n",
       " ('инсту', 0.7789162397384644),\n",
       " ('Ии', 0.7788558006286621),\n",
       " ('недоступна', 0.7754437923431396),\n",
       " ('покажи', 0.7736333608627319),\n",
       " ('берешь', 0.7618379592895508),\n",
       " ('Ане', 0.7555817365646362),\n",
       " ('Эту', 0.7530922293663025),\n",
       " ('фотки)', 0.7501952648162842),\n",
       " ('Ване', 0.7494977116584778)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_1.wv.most_similar(positive=[\"Юля\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
